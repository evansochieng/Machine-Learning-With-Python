{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503ebabd",
   "metadata": {},
   "source": [
    "### Project field\n",
    "- Use Reinforcement Learning with Q-learning to find solutions to this field.\n",
    "![Field](img/field-2.png \"Field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1bb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "266beae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the field\n",
    "\n",
    "class Field:\n",
    "    #create a list of values to represent all the states\n",
    "    def __init__(self):\n",
    "        self.states = [[-1, 0, 0, 0, 0, 0, 0, 0, 0 , 0, 0],\n",
    "                       [-1, 0, 0, 0, 0, 0, 0, 1, 0, 0 , 0],\n",
    "                       [0, 0, 0, 0, 0, 0, 0, 0, 0 , 0, 0]]\n",
    "        self.state = (random.randrange(0, len(self.states)), random.randrange(0, len(self.states[0])))\n",
    "        #represent this as a (x, y) coordinate:x and y are the number of rows and columns respectively\n",
    "     \n",
    "    #check if current state has non-negative numbers\n",
    "    def done(self):\n",
    "        if self.states[self.state[0]][self.state[1]] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #Set a list to all possible actions, i.e. actions = [0, 1, 2, 3]\n",
    "    #Then check if state is in a position where a possible actions should be removed.\n",
    "    #Finally, return the remaining actions\n",
    "    #actions: 0 -> left\n",
    "    #actions: 1 -> right\n",
    "    #actions: 2 -> up\n",
    "    #actions: 3 -> down\n",
    "    def get_possible_actions(self):\n",
    "        actions = [0, 1, 2, 3]\n",
    "        if self.state[0] == 0 : #if in top row\n",
    "            actions.remove(2)\n",
    "        if self.state[0] == len(self.states) - 1: #if in bottom row\n",
    "            actions.remove(3)\n",
    "        if self.state[1] == 0: #if in first column\n",
    "            actions.remove(0)\n",
    "        if self.state[1] ==len(self.states[0]) - 1: #if in last column\n",
    "            actions.remove(1)\n",
    "        return actions\n",
    "    \n",
    "    #Get the current state\n",
    "    #Check if move is illegal, then return current state and -10 in reward\n",
    "    #Otherwise opdate state and return the reward according to new state\n",
    "    def update_next_state(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:\n",
    "            if y == 0: # if in first column\n",
    "                return self.state, -10\n",
    "            self.state = x, y - 1 #else move left\n",
    "        if action == 1:\n",
    "            if y == len(self.states[0]) - 1: #if in last column\n",
    "                return self.state, -10\n",
    "            self.state = x, y + 1 #else move right\n",
    "        if action == 2:\n",
    "            if x == 0: #if in first row\n",
    "                return self.state, -10\n",
    "            self.state = x - 1, y #else move up\n",
    "        if action == 3:\n",
    "            if x == len(self.states) - 1: #if in last row\n",
    "                return self.state, -10\n",
    "            self.state = x + 1, y #else move down\n",
    "        reward = self.states[self.state[0]][self.state[1]]\n",
    "        return self.state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f05f3e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 3), False, [0, 1, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = Field()\n",
    "field.state, field.done(), field.get_possible_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e654f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 3), False, [0, 1, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field.update_next_state(2)\n",
    "field.state, field.done(), field.get_possible_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52cba4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "field = Field()\n",
    "#create a q_table initialized to all 0\n",
    "q_table = np.zeros((len(field.states), len(field.states[0]), 4))\n",
    "\n",
    "alpha = .5\n",
    "gamma = .5\n",
    "epsilon = .5\n",
    "\n",
    "for _ in range(10000):\n",
    "    field = Field() #create a field\n",
    "    while not field.done(): #while not done\n",
    "        actions = field.get_possible_actions() #get the possible actions\n",
    "        #With probability epsilon take a random action, otherwise take the best action\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions) #random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[field.state]) #best action\n",
    "            \n",
    "        cur_x, cur_y = field.state #get current state\n",
    "        (next_x, next_y), reward = field.update_next_state(action) #Update next state and get it and the reward\n",
    "        \n",
    "        q_table[cur_x, cur_y, action] = (1 - alpha)*q_table[cur_x, cur_y, action] + alpha*(reward + gamma*np.max(q_table[next_x, next_y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c0f626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solve the task\n",
    "path = np.zeros((3, 11))\n",
    "field = Field() #craete a field\n",
    "steps = 1 #to count the steps\n",
    "path[field.state[0]][field.state[1]] = np.nan #Assign the start state in the path to np.nan\n",
    "\n",
    "while not field.done(): #while not solved\n",
    "    action = np.argmax(q_table[field.state]) #get the action to take\n",
    "    \n",
    "    (next_x, next_y), _ = field.update_next_state(action) #get the next state\n",
    "    path[next_x][next_y] = steps #update path with steps\n",
    "    \n",
    "    steps += 1 #increment steps by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c30f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1., nan,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the path\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The training phase could just take random actions\n",
    "#This could be possible if we set 'epsilon' to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
